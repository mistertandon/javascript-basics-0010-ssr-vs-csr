 - [Understand the Softmax Function in Minutes](https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d)

 - [Derivative of the Sigmoid function](https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e)

 - [Log Loss](http://wiki.fast.ai/index.php/Log_Loss)

 - [Step-by-Step Tutorial on Linear Regression with Stochastic Gradient Descent](https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843)

 - [Gradient Descent: All You Need to Know](https://hackernoon.com/gradient-descent-aynk-7cbe95a778da)

 - [Machine Learning week 1: Cost Function, Gradient Descent and Univariate Linear Regression](https://medium.com/@lachlanmiller_52885/machine-learning-week-1-cost-function-gradient-descent-and-univariate-linear-regression-8f5fe69815fd)

 - [stochastic gradient descent (SGD)](https://medium.com/@hakobavjyan/stochastic-gradient-descent-sgd-10ce70fea389)

 - [Logistic Regression â€” Detailed Overview](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)

 - [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)

 - [Neural Network -Activation functions](https://medium.com/datadriveninvestor/neural-networks-activation-functions-e371202b56ff)

 - [Shannon Entropy, Information Gain, and Picking Balls from Buckets](https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4)

 - [Demystifying Entropy](https://towardsdatascience.com/demystifying-entropy-f2c3221e2550)

 - [Stochastic Gradient Descent with momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)

 - [NEURAL NETWORKS](https://natureofcode.com/book/chapter-10-neural-networks/)

 - [Understanding softmax and the negative log-likelihood](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)

 - [Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)

 - [Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec)